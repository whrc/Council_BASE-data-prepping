---
title: "Optimizing RF ntree_OOB_NEE and CH4" #using OOB (out of bag error) to determine optimum number of trees for training RF model, checked using RMSE 
output: html_document
date: "2025-08-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=FALSE}
rm(list = ls())

library(data.table)
library(ggplot2)
library(cowplot)
library(openair)
library(plotrix)
library(signal)
library(svMisc)
library(zoo)
library(stringr)
library(plyr)
library(viridis)
library(lubridate)
library(tidyverse)
library(gridExtra)
library(plotly)
library(RColorBrewer)
library(pracma)
library(dplyr)


Sys.setenv(TZ='UTC')
```

# Set working directory and then Load data

TO DO: re-running all these with SWC_3 (tussock) version of dataset - KK 1/7/2025 **use the era dataset** 

```{r}
#changed to "council_gapfilled_clean_2017_2023_for analysis.2.csv" --> .2 indicates updated dataset with SWC_3 tussock 

#half-hourly dataframe --> gapfilled 
#df = fread('C:/Users/kkent/Documents/Council Data/Council BASE gapfilling/council_gapfilled_clean_2017_2023_for analysis.2.csv',na.strings = c('-9999','NA','NaN','NAN','-7999'))


#daily avg dataframe --> gapfilled 
#df_avg = fread('C:/Users/kkent/Documents/Council Data/Council BASE gapfilling/council_AVG_gapfilled_clean_2017_2023_for analysis.2.csv',na.strings = c('-9999','NA','NaN','NAN','-7999'))


```

#ERA dataset 
####Prepping the era dataset to use to train RF too -- calc vpd and RF for SWC gapfilling, just like in the processing steps 
```{r}
#ERA-5 dataset for RF training - just like in step 2 of the processing code -- uses the same dataset used in the beginning of step 2 
df_era = fread("C:/Users/kkent/Documents/Council Data/Council BASE gapfilling/council_2016_2023_era.csv") #using the re-cleaned df from Dani 

#subset out by which has complete air t data 
df_era = df_era[complete.cases(df_era$airt.eramod),]

#create subset of the variables wanted for training RF 
df_era2 = data.frame(df_era$date,df_era$FC.c,df_era$FCH4.c,
                 df_era$airt.eramod, df_era$wd, df_era$cardinal_direction, df_era$rh.eramod,df_era$rad.eramod,
                 df_era$ws.eramod,df_era$tsoil.eramod,#df_era$SWC_1_1_1.c,df_era$SWC_2_1_1.c, 
                 df_era$SWC_3_1_1.c, #SWC 3 = the tussock location 
                 df_era$h.eramod,df_era$le.eramod)

#rename for easier names
names(df_era2) = c('date','nee',"fch4",'tair', 'wd', 'cardDir','rh','rg','ws','tsoil','swc','h','le')

#make card dir as factor - 8 levels 
df_era2$cardDir <- as.factor(df_era2$cardDir)

#calculate VPD from air t and RH
svp = 610.7*10^((7.5*df_era2$tair)/(237.3+df_era2$tair))
df_era2$vpd = ((100 - df_era2$rh)/100)*svp  

#Train RF to fill in SWC (using SWC3, tussocklocation*)

set.seed(123)#sets the start point of models, good for repeatability
cc = df_era2[complete.cases(df_era2$swc),]#create a gap free data set of the target variable

#use 80% of data set as training set and 20% as test set
sample = sample(c(TRUE, FALSE), nrow(cc), replace=TRUE, prob=c(0.8,0.2))
train  = cc[sample, ]
test   = cc[!sample, ]

#compare to ensure all data sets are representative of total data
hist(cc$swc)
hist(train$swc)
hist(test$swc)

#run random forest to predict missing SWC values ################################################
library(randomForest)

rf.swc = randomForest(formula = swc ~ tair + rh + rg + ws + wd + tsoil + vpd + le + h,data = train,ntree = 150)
#tried various tree numbers, no sig improvement after 300....very minimal diff between 150 and 300, and 150 looks slightly better.

#predict it on the full data set
swc.rf = predict(object = rf.swc,newdata = df_era2)
df_era2$swc.rf = swc.rf 

#time series plot of gap filled vs real
ggplot(data = df_era2)+
  geom_point(aes(date,swc.rf*100,col='RF'))+
  geom_point(aes(date,swc*100,col='Measured'))+
  scale_y_continuous("SWC (%)")


#validation dataset from only test data
val = merge(test,df_era2,by = "date",all.x = T)

ggplot(data = val,aes(swc.rf,swc.x))+theme_bw()+geom_abline(slope = 1,intercept = 0,col='red')+
  geom_point(alpha=0.1)+
  scale_x_continuous(limits = c(0,70),"RF SWC (%)")+
  scale_y_continuous(limits = c(0,70),"Measured SWC (%)")

#summary stats on gap filling
summary(lm(val$swc.x ~ val$swc.rf)) #R2 = 0.97, slope = 1.02

#create final gap free soil moisture
df_era2$swc = ifelse(is.na(df_era2$swc),df_era2$swc.rf,df_era2$swc)

```





#ntree optimization for rf_all_era2 model (which includes the swc:temp interaction and fch4 constraints)

#OOB ntrees for Methane 

```{r}
#interaction term in ERA5 dataset, for reference: TS_SWC_interact

#for reference, since we already subsetted a constrained dataset in previous code chunks 
# cc = df_era2[complete.cases(df_era2$fch4),]
# cc = subset(cc,cc$fch4 < 60)
# cc = subset(cc,cc$fch4 > -25)

#create training dataset - 80% to train, 20% to test 
# sample.ch4 = sample(c(TRUE, FALSE), nrow(cc), replace=TRUE, prob=c(0.8,0.2))
# train.ch4  = cc[sample.ch4, ]
# test.ch4   = cc[!sample.ch4, ]

# Function to track OOB error versus number of trees with progress bar --> going to try up to 1k because that's what Jackie found was best in her YK paper 
oob_error_plot <- function(data, max_trees = 1000, step = 100) {
    trees <- seq(50, max_trees, by = step)
    oob_errors <- numeric(length(trees))
    
    # Create progress bar
    pb <- txtProgressBar(min = 0, max = length(trees), style = 3)
    
    for(i in seq_along(trees)) {
        rf_temp <- randomForest(fch4 ~ tair + rh + rg + ws + wd + tsoil + 
                              vpd + swc + h + le + TS_SWC_interact,
                              data = data,
                              ntree = trees[i])
        oob_errors[i] <- rf_temp$mse[trees[i]]
        
        # Update progress bar
        setTxtProgressBar(pb, i)
    }
    
    # Close progress bar
    close(pb)
    
    # Plot OOB error vs number of trees
    plot(trees, oob_errors, type = "l",
         xlab = "Number of Trees",
         ylab = "OOB Mean Squared Error",
         main = "OOB Error vs Number of Trees")
    
    # Add points to see exact values
    points(trees, oob_errors, pch = 16)
    
    # Return optimal number of trees and error data
    opt_trees <- trees[which.min(oob_errors)]
    return(list(optimal_trees = opt_trees, 
                errors = data.frame(trees = trees, oob = oob_errors)))
}

# Find optimal number of trees
opt_results <- oob_error_plot(train.ch4)
print(paste("Optimal number of trees:", opt_results$optimal_trees))

# Print OOB errors for each number of trees
print("OOB errors for each number of trees:")
print(opt_results$errors)

#REsults: "Optimal number of trees: 950"
# trees      oob
# 50	     46.29153			
# 150	     44.41954			
# 250	     44.35895			
# 350	      44.13764			
# 450	     44.08307			
# 550	     43.97322			
# 650	     44.06375			
# 750	     43.95684			
# 850	     43.94687			
# 950	     43.87470	
#improvement drops significantly after around 550/600 trees, so could likely use 550/600 trees just fine with little loss of accuracy

```

#Optimal number of trees for NEE?
####OOB for NEE
```{r}
set.seed(123)
cc_nee = df_era2[complete.cases(df_era2$nee),] #complete NEE data only 
cc_nee = subset(cc_nee,cc_nee$nee < 12 & cc_nee$nee > -14) 


#use 80% of data set as training set and 20% as test set
sample.nee = sample(c(TRUE, FALSE), nrow(cc_nee), replace=TRUE, prob=c(0.8,0.2))
train.nee  = cc_nee[sample.nee, ]
test.nee   = cc_nee[!sample.nee, ]

#Make sure the patterns here look similar so you know the training and testing datasets are representative of the overall dataset 
hist(cc_nee$nee)
hist(train.nee$nee)
hist(test.nee$nee)

library(randomForest)

# Function to track OOB error versus number of trees with progress bar --> going to try up to 1k because that's what Jackie found was best in her YK paper 
oob_error_plot_nee <- function(data, max_trees = 1000, step = 100) {
    trees <- seq(50, max_trees, by = step)
    oob_errors_nee <- numeric(length(trees))
    
    # Create progress bar
    pb <- txtProgressBar(min = 0, max = length(trees), style = 3)
    
    for(i in seq_along(trees)) {
        rf_nee <- randomForest(nee ~ tair + rh + rg + ws + wd + tsoil + 
                              vpd + swc + h + le,
                              data = data,
                              ntree = trees[i])
        oob_errors_nee[i] <- rf_nee$mse[trees[i]]
        
        # Update progress bar
        setTxtProgressBar(pb, i)
    }
    
    # Close progress bar
    close(pb)
    
    # Plot OOB error vs number of trees
    plot(trees, oob_errors_nee, type = "l",
         xlab = "Number of Trees",
         ylab = "OOB Mean Squared Error",
         main = "OOB Error vs Number of Trees for NEE (no temp swc interaction)")
    
    # Add points to see exact values
    points(trees, oob_errors_nee, pch = 16)
    
    # Return optimal number of trees and error data
    opt_trees_nee <- trees[which.min(oob_errors_nee)]
    return(list(optimal_trees_nee = opt_trees_nee, 
                errors = data.frame(trees = trees, oob = oob_errors_nee)))
}

# Find optimal number of trees
opt_results_nee <- oob_error_plot_nee(train.nee)
print(paste("Optimal number of trees:", opt_results_nee$optimal_trees_nee))

# Print OOB errors for each number of trees
print("OOB errors for each number of trees for NEE:")
print(opt_results_nee$errors)

#Results: 
#optimal number of trees at 850, but with minimal improvement from 550 (2.124 â†’ 2.120) --> Could likely use 550-600 trees without significant loss in accuracy

# trees  oob
# 50	2.255787			
# 150	2.150538			
# 250	2.128528			
# 350	2.131406			
# 450	2.128552			
# 550	2.123629			
# 650	2.124292			
# 750	2.121626			
# 850	2.120393	****		
# 950	2.121901			


```



#OOB ntree for SWC in RF

#Optimal number of trees for SWC (SWC3)

```{r}

orig = Sys.time()#starting timer

set.seed(123)
cc_swc = df_era2[complete.cases(df_era2$swc),] #complete swc data only 

#use 80% of data set as training set and 20% as test set
sample.swc = sample(c(TRUE, FALSE), nrow(cc_swc), replace=TRUE, prob=c(0.8,0.2))
train.swc  = cc_swc[sample.swc, ]
test.swc   = cc_swc[!sample.swc, ]

#Make sure the patterns here look similar so you know the training and testing datasets are representative of the overall dataset 
hist(cc_swc$swc)
hist(train.swc$swc)
hist(test.swc$swc)

library(randomForest)

# Function to track OOB error versus number of trees with progress bar 
oob_error_plot_swc <- function(data, max_trees = 1000, step = 100) {
    trees <- seq(100, max_trees, by = step)
    oob_errors_swc <- numeric(length(trees))
    
    # Create progress bar
    pb <- txtProgressBar(min = 0, max = length(trees), style = 3)

   
    for(i in seq_along(trees)) {
        rf_swc <- randomForest(swc ~ tair + rh + rg + ws + wd + tsoil + 
                              vpd + h + le,
                              data = data,
                              ntree = trees[i])
        oob_errors_swc[i] <- rf_swc$mse[trees[i]]
        
        # Update progress bar
        setTxtProgressBar(pb, i)
    }
    
    # Close progress bar
    close(pb)
    
    # Plot OOB error vs number of trees
    plot(trees, oob_errors_swc, type = "l",
         xlab = "Number of Trees",
         ylab = "OOB Mean Squared Error",
         main = "OOB Error vs Number of Trees for SWC (SWC3)")
    
    # Add points to see exact values
    points(trees, oob_errors_swc, pch = 16)
    
    # Return optimal number of trees and error data
    opt_trees_swc <- trees[which.min(oob_errors_swc)]
    return(list(optimal_trees_swc = opt_trees_swc, 
                errors = data.frame(trees = trees, oob = oob_errors_swc)))
}

# Find optimal number of trees
opt_results_swc <- oob_error_plot_swc(train.swc)
print(paste("Optimal number of trees:", opt_results_swc$optimal_trees_swc))

# Print OOB errors for each number of trees
print("OOB errors for each number of trees for swc:")
print(opt_results_swc$errors)

#Results: optimal number of trees is 800*
# trees oob
# 100	14.03046			
# 200	13.52641			
# 300	13.43767			
# 400	13.33447			
# 500	13.40129			
# 600	13.34920			
# 700	13.31430			
# 800	13.23051			
# 900	13.31164			
# 1000	13.28281	


Sys.time() - orig #stop timer --> took 4.3 hours 


```